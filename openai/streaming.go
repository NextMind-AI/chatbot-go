package openai

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"strings"
	"time"
	"reflect"
	"sync"

	"github.com/NextMind-AI/chatbot-go/aiutils"
	"github.com/NextMind-AI/chatbot-go/elevenlabs"
	"github.com/NextMind-AI/chatbot-go/redis"
	"github.com/NextMind-AI/chatbot-go/vonage"

	"github.com/openai/openai-go"
	"github.com/rs/zerolog/log"
)

// streamingConfig holds the configuration for a streaming chat completion request.
type streamingConfig struct {
	userID           string
	userName         string
	chatHistory      []redis.ChatMessage
	vonageClient     *vonage.Client
	redisClient      *redis.Client
	elevenLabsClient *elevenlabs.Client
	toNumber         string
}

// ProcessChatStreaming processes a chat conversation with streaming response.
// It sends messages to the user via WhatsApp as they are generated by the AI.
// This method does not use any tools.
func (c *Client) ProcessChatStreaming(
	ctx context.Context,
	userID string,
	userName string,
	chatHistory []redis.ChatMessage,
	vonageClient *vonage.Client,
	redisClient *redis.Client,
	elevenLabsClient *elevenlabs.Client,
	toNumber string,
) error {
	config := streamingConfig{
		userID:           userID,
		userName:         userName,
		chatHistory:      chatHistory,
		vonageClient:     vonageClient,
		redisClient:      redisClient,
		elevenLabsClient: elevenLabsClient,
		toNumber:         toNumber,
	}
	return c.processStreamingChat(ctx, config)
}

// ProcessChatStreamingWithTools processes a chat conversation with the new two-step approach:
// First, it uses a sleep analyzer to determine wait time, then generates the actual response.
func (c *Client) ProcessChatStreamingWithTools(
	ctx context.Context,
	userID string,
	userName string,
	chatHistory []redis.ChatMessage,
	vonageClient *vonage.Client,
	redisClient *redis.Client,
	elevenLabsClient *elevenlabs.Client,
	toNumber string,
) error {
	config := streamingConfig{
		userID:           userID,
		userName:         userName,
		chatHistory:      chatHistory,
		vonageClient:     vonageClient,
		redisClient:      redisClient,
		elevenLabsClient: elevenLabsClient,
		toNumber:         toNumber,
	}
	return c.ExecuteSleepAndRespond(ctx, config)
}

// processStreamingChat handles the core streaming logic.
// Since tools are no longer used, this simply converts history and streams the response.
func (c *Client) processStreamingChat(ctx context.Context, config streamingConfig) error {
	// converte histórico p/ mensagens
	messages := c.convertChatHistoryWithUserName(config.chatHistory, config.userName, config.userID)

	updatedMessages, err := c.handleToolCalls(ctx, messages, config.userID)
	if err != nil {
		// se é um erro destinado ao usuário -> enviar imediatamente e não iniciar streaming
		var userErr *aiutils.ToolCallUserError
		if errors.As(err, &userErr) {
			// construir Message simples
			userMsg := Message{
				Content: userErr.UserMessage,
				Type:    "text",
			}
			// enviamos direto e consideramos o trabalho concluído (retornamos nil)
			if sendErr := c.sendTextMessage(ctx, config, userMsg, 0); sendErr != nil {
				log.Error().
					Err(sendErr).
					Str("user_id", config.userID).
					Msg("failed to send user-facing error message")
				return sendErr
			}
			log.Info().
				Str("user_id", config.userID).
				Msg("Sent user-facing tool error message; skipping streaming")
			return nil
		}
		// erro normal
		log.Error().
			Err(err).
			Str("user_id", config.userID).
			Msg("❌ failed to process tool calls before streaming")
		return err
	}

	// Se handleToolCalls não alterou as mensagens, updatedMessages == messages (mesmo conteúdo/len)
	// Se alterou (por exemplo adicionou ToolMessage com os resultados), então usaremos streaming sem tools
	didHandleToolCalls := len(updatedMessages) > len(messages)

	if didHandleToolCalls {
		log.Info().
			Str("user_id", config.userID).
			Int("original_messages", len(messages)).
			Int("updated_messages", len(updatedMessages)).
			Msg("🔁 Tool calls processed — starting streaming WITHOUT tools to avoid duplicate calls")
		// use the streaming variant that does not include tools
		return c.streamResponseWithoutTools(ctx, config, updatedMessages)
	}

	// nenhum tool call foi processado: normal streaming (com tools) — mantém o comportamento original
	log.Info().
		Str("user_id", config.userID).
		Msg("▶️ No tool calls processed — starting normal streaming (with tools)")
	return c.streamResponse(ctx, config, updatedMessages)
}

// streamResponse creates a streaming chat completion and sends messages via WhatsApp as they arrive.
// It handles the parsing of streamed JSON and manages message deduplication with guaranteed ordering.
func (c *Client) streamResponse(
	ctx context.Context,
	config streamingConfig,
	messages []openai.ChatCompletionMessageParamUnion,
) error {
	schemaParam := createSchemaParam()

	// Prepare tools for the request
	tools := []openai.ChatCompletionToolParam{}
	for _, tool := range c.tools {
		tools = append(tools, tool.Definition)
	}

	params := openai.ChatCompletionNewParams{
		Messages: messages,
		ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
			OfJSONSchema: &openai.ResponseFormatJSONSchemaParam{JSONSchema: schemaParam},
		},
		Model: c.model,
	}

	// Add tools if any are defined
	if len(tools) > 0 {
		params.Tools = tools
	}

	log.Info().
		Str("user_id", config.userID).
		Msg("Starting new streaming response")

	stream := c.client.Chat.Completions.NewStreaming(ctx, params)

	parser := NewStreamingJSONParser()
	var fullContent strings.Builder
	sentMessages := make(map[int]bool)

	messageQueue := make(chan messageWithIndex, 100)
	done := make(chan struct{})

	go func() {
		defer close(done)
		log.Info().
			Str("user_id", config.userID).
			Msg("Started goroutine for sequential message sending")
		c.sendMessagesSequentially(ctx, config, messageQueue)
		log.Info().
			Str("user_id", config.userID).
			Msg("Goroutine for sequential message sending finished")
	}()

	totalMessagesQueued := 0

	for stream.Next() {
		evt := stream.Current()
		log.Debug().
			Str("user_id", config.userID).
			Msg("Received new stream event")
		if len(evt.Choices) > 0 {
			content := evt.Choices[0].Delta.Content
			fullContent.WriteString(content)

			log.Debug().
				Str("user_id", config.userID).
				Str("content_chunk", content).
				Msg("Appended content chunk to fullContent")

			newMessages := parser.AddChunk(content)

			for i, msg := range newMessages {
				messageIndex := parser.MsgCount - len(newMessages) + i
				if !sentMessages[messageIndex] {
					sentMessages[messageIndex] = true
					totalMessagesQueued++

					log.Info().
						Str("user_id", config.userID).
						Int("message_index", messageIndex).
						Int("total_queued", totalMessagesQueued).
						Str("content", msg.Content).
						Str("type", msg.Type).
						Msg("Queueing streamed message for sequential sending")

					select {
					case messageQueue <- messageWithIndex{
						message: msg,
						index:   messageIndex,
					}:
						log.Debug().
							Str("user_id", config.userID).
							Int("message_index", messageIndex).
							Msg("Message successfully sent to messageQueue")
					case <-ctx.Done():
						log.Warn().
							Str("user_id", config.userID).
							Int("total_queued", totalMessagesQueued).
							Msg("Context done while sending to messageQueue, closing queue")
						close(messageQueue)
						<-done
						return ctx.Err()
					}
				}
			}
		}
	}

	log.Info().
		Str("user_id", config.userID).
		Int("total_messages_queued", totalMessagesQueued).
		Msg("Stream finished, closing messageQueue")

	close(messageQueue)
	<-done

	if err := stream.Err(); err != nil {
		log.Error().
			Str("user_id", config.userID).
			Err(err).
			Msg("Stream encountered error")
		return err
	}

	log.Info().
		Str("user_id", config.userID).
		Msg("Finalizing streaming response")
	return c.finalizeStreamingResponse(config.userID, fullContent.String(), config.redisClient)
}


func (c *Client) streamResponseWithoutTools(
	ctx context.Context,
	config streamingConfig,
	messages []openai.ChatCompletionMessageParamUnion,
) error {
	logger := log.With().Str("user_id", config.userID).Logger()
	logger.Info().Msg("Starting new streaming response (without tools)")

	// sanitize para evitar role:"tool"
	sanitizedMessages := aiutils.SanitizeMessagesForNoTools(messages)

	// abrir stream
	startStream := time.Now()
	stream := c.client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{
		Messages: sanitizedMessages,
		ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
			OfJSONSchema: &openai.ResponseFormatJSONSchemaParam{JSONSchema: createSchemaParam()},
		},
		Model: c.model,
	})
	logger.Info().Dur("stream_creation_elapsed", time.Since(startStream)).Msg("Streaming client created (without tools)")

	parser := NewStreamingJSONParser()
	var fullContent strings.Builder
	sentMessages := make(map[int]bool)
	totalMessagesQueued := 0

	// local type para fila (não polui o package)
	type queuedMsg struct {
		msg   Message
		index int
	}

	// -------------------------
	// Configuráveis / tunáveis
	// -------------------------
	const (
		messageQueueBuffer = 3000                 // buffer do canal (aumente se tiver memória)
		maxBatch           = 16                   // quantos itens enviar num batch (aumentei de 8 para 12)
		batchTimeout       = 120 * time.Millisecond
		perSendTimeout     = 4 * time.Second
		audioConcurrency   = 4                    // quantos envios de áudio paralelos (ajuste conforme CPU/banda)
		queueRetryDelay    = 120 * time.Millisecond
		queueRetryTimeout  = 250 * time.Millisecond
	)

	// buffer generoso para evitar bloqueios do produtor
	messageQueue := make(chan queuedMsg, messageQueueBuffer)
	done := make(chan struct{})

	// Goroutine consumidora — batching + timeouts + envio (usa seus helpers sendTextMessage/sendAudioMessage)
	go func() {
		defer close(done)
		logger.Info().Msg("Started inline goroutine for sequential message sending (without tools)")

		batch := make([]queuedMsg, 0, maxBatch)
		timer := time.NewTimer(batchTimeout)
		defer timer.Stop()

		// semáforo para limitar concorrência de envios de áudio
		audioSem := make(chan struct{}, audioConcurrency)

		flush := func() {
			if len(batch) == 0 {
				return
			}

			// separar audio x text
			textParts := make([]string, 0, len(batch))
			audioItems := make([]queuedMsg, 0)

			for _, it := range batch {
				if strings.ToLower(it.msg.Type) == "audio" {
					audioItems = append(audioItems, it)
				} else {
					textParts = append(textParts, it.msg.Content)
				}
			}

			// enviar texto combinado (se houver) — único envio por batch
			if len(textParts) > 0 {
				combined := strings.Join(textParts, "\n\n")
				pm := Message{Content: combined, Type: "text"}

				sendCtx, cancel := context.WithTimeout(ctx, perSendTimeout)
				start := time.Now()
				if err := c.sendTextMessage(sendCtx, config, pm, -1); err != nil {
					logger.Error().Err(err).Int("batch_count", len(textParts)).Dur("elapsed", time.Since(start)).Msg("Failed to send text batch")
				} else {
					logger.Debug().Int("batch_count", len(textParts)).Dur("elapsed", time.Since(start)).Msg("Sent text batch")
				}
				cancel()
			}

			// enviar áudios individualmente em paralelo limitado
			if len(audioItems) > 0 {
				var wg sync.WaitGroup
				wg.Add(len(audioItems))
				for _, ai := range audioItems {
					ai := ai // captura
					// adquirir semáforo ou bloquear até que haja slot
					audioSem <- struct{}{}
					go func() {
						defer wg.Done()
						defer func() { <-audioSem }()
						sendCtx, cancel := context.WithTimeout(ctx, perSendTimeout)
						start := time.Now()
						if err := c.sendAudioMessage(sendCtx, config, ai.msg, ai.index); err != nil {
							logger.Error().Err(err).Int("message_index", ai.index).Dur("elapsed", time.Since(start)).Msg("Failed to send audio message (parallel)")
						} else {
							logger.Debug().Int("message_index", ai.index).Dur("elapsed", time.Since(start)).Msg("Sent audio message (parallel)")
						}
						cancel()
					}()
				}
				// aguarda todos concluírem antes de continuar; evita acumular batches sem controle
				wg.Wait()
			}

			// reset batch
			batch = batch[:0]
		}

		// loop consumidor
		for {
			// cheque cancelamento rapidamente
			if ctx.Err() != nil {
				flush()
				logger.Info().Msg("Context canceled in consumer; flushing and exiting consumer goroutine")
				return
			}

			select {
			case <-ctx.Done():
				flush()
				return
			case item, ok := <-messageQueue:
				if !ok {
					// canal fechado: flush final e sair
					flush()
					logger.Info().Msg("messageQueue closed - consumer exiting")
					return
				}
				batch = append(batch, item)
				if len(batch) >= maxBatch {
					flush()
					// reset timer de forma segura
					if !timer.Stop() {
						select {
						case <-timer.C:
						default:
						}
					}
					timer.Reset(batchTimeout)
				}
			case <-timer.C:
				flush()
				timer.Reset(batchTimeout)
			}
		}
	}()

	// produtor: ler stream, parsear JSON incremental, enfileirar mensagens
	ProducerLoop:
		for stream.Next() {
			evt := stream.Current()

			// defensive check: evitar panic se não vier nada
			if evt.Choices == nil || len(evt.Choices) == 0 {
				continue
			}

			delta := evt.Choices[0].Delta
			content := delta.Content
			if content == "" {
				continue
			}

			fullContent.WriteString(content)
			// reduzir log no hot-path — só log quando chunks grandes
			if len(content) > 512 {
				logger.Debug().Int("chunk_len", len(content)).Msg("Received large content chunk (without tools)")
			}

			newMessages := parser.AddChunk(content) // espera-se []Message
			if len(newMessages) > 0 {
				logger.Info().Int("new_messages_count", len(newMessages)).Msg("Parser found new messages in chunk")
			}

			for i, msg := range newMessages {
				messageIndex := parser.MsgCount - len(newMessages) + i
				if sentMessages[messageIndex] {
					continue
				}
				sentMessages[messageIndex] = true
				totalMessagesQueued++

				// Tentativa não-bloqueante com retry curto; evita travar pipeline se queue cheia
				select {
				case messageQueue <- queuedMsg{msg: msg, index: messageIndex}:
					// enfileirado com sucesso
				default:
					// fila cheia: tentar uma vez bloqueando por curto tempo, depois dropar
					timer := time.NewTimer(queueRetryDelay)
					select {
					case messageQueue <- queuedMsg{msg: msg, index: messageIndex}:
						if !timer.Stop() {
							<-timer.C
						}
					case <-timer.C:
						// segunda tentativa falhou: dropar mensagem (evitar deadlock)
						logger.Warn().Int("message_index", messageIndex).Msg("Dropped message due to full queue (without tools)")
					case <-ctx.Done():
						timer.Stop()
						logger.Warn().Msg("Context canceled while queuing message; closing queue and exiting")
						close(messageQueue)
						<-done
						return ctx.Err()
					}
				}
			}
			// cheque cancelamento de fora do loop também
			if ctx.Err() != nil {
				break ProducerLoop
			}
		}

	// stream terminou: fechar fila e esperar consumer encerrar
	logger.Info().Int("total_messages_queued", totalMessagesQueued).Msg("Stream finished, closing messageQueue (without tools)")
	close(messageQueue)
	<-done

	// verificar erro do stream
	if err := stream.Err(); err != nil {
		logger.Error().Err(err).Msg("Stream encountered error (without tools)")
		return err
	}

	// se nada foi enfileirado, avisar (ajuda debug)
	if totalMessagesQueued == 0 {
		logger.Warn().Msg("WARNING: No messages were queued during streaming - bot may not have responded")
	}

	logger.Info().Msg("Finalizing streaming response (without tools)")
	return c.finalizeStreamingResponse(config.userID, fullContent.String(), config.redisClient)
}


// remove mensagens antigas até o payload ficar <= maxBytes
func shrinkMessagesByBytes(msgs []openai.ChatCompletionMessageParamUnion, maxBytes int) []openai.ChatCompletionMessageParamUnion {
    // if it's already small, return
    b, _ := json.Marshal(msgs)
    if len(b) <= maxBytes {
        return msgs
    }
    // remove mensagens mais antigas até caber
    start := 0
    for start < len(msgs) {
        subset := msgs[start:]
        b2, _ := json.Marshal(subset)
        if len(b2) <= maxBytes || len(subset) == 0 {
            return subset
        }
        start++
    }
    return msgs // fallback (não deve acontecer)
}

// handleToolCalls - versão corrigida para evitar passar map[string]any para openai.ToolMessage
func (c *Client) handleToolCalls(
	ctx context.Context,
	messages []openai.ChatCompletionMessageParamUnion,
	userID string,
) ([]openai.ChatCompletionMessageParamUnion, error) {

	// Preparar ferramentas registradas
	tools := make([]openai.ChatCompletionToolParam, 0, len(c.tools))
	for _, tool := range c.tools {
		tools = append(tools, tool.Definition)
	}

	// Garantir histórico dentro do limite
	messages = shrinkMessagesByBytes(messages, 1024) // corta histórico se precisar

	startReq := time.Now()
	payloadBytes, _ := json.Marshal(messages)
	log.Info().
		Str("user_id", userID).
		Int("tool_count", len(tools)).
		Int("payload_bytes", len(payloadBytes)).
		Msg("🔧 Calling AI with custom tools - starting request")

	// Chamada streaming
	reqStart := time.Now()
	stream := c.client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{
		Messages: messages,
		Tools:    tools,
		Model:    c.model,
	})
	if stream == nil {
		return nil, fmt.Errorf("failed to create streaming request: stream is nil")
	}
	log.Info().Msg("Acabou o Streaming!")
	defer func() { _ = stream.Close() }()

	var finalMessage openai.ChatCompletionMessage
	var collectedContent strings.Builder

	// Ler eventos do stream
	for stream.Next() {
		chunk := stream.Current()
		if len(chunk.Choices) == 0 {
			continue
		}
		choice := chunk.Choices[0]
		delta := choice.Delta
		// pular se não tiver nada útil
		if delta.Content == "" && delta.Role == "" && len(delta.ToolCalls) == 0 {
			continue
		}

		if delta.Content != "" {
			collectedContent.WriteString(delta.Content)
			fmt.Print(delta.Content)
		}

		// atribuição robusta do Role (reflection)
		if delta.Role != "" {
			rv := reflect.ValueOf(&finalMessage).Elem()
			f := rv.FieldByName("Role")
			if f.IsValid() && f.CanSet() {
				switch f.Kind() {
				case reflect.String:
					f.SetString(delta.Role)
				case reflect.Ptr:
					if f.Type().Elem().Kind() == reflect.String {
						s := delta.Role
						f.Set(reflect.ValueOf(&s))
					} else {
						if reflect.TypeOf(delta.Role).AssignableTo(f.Type()) {
							f.Set(reflect.ValueOf(delta.Role))
						}
					}
				default:
					if reflect.TypeOf(delta.Role).AssignableTo(f.Type()) {
						f.Set(reflect.ValueOf(delta.Role))
					}
				}
			}
		}

		// tool calls: construir ToolCall entries
		for _, tc := range delta.ToolCalls {
			if tc.ID == "" || tc.Type == "" || tc.Function.Name == "" {
				log.Debug().Interface("tool_call", tc).Msg("skipping malformed tool call chunk")
				continue
			}

			// Construir item sem o Type (assign via reflection se necessário)
			item := openai.ChatCompletionMessageToolCall{
				ID: tc.ID,
				Function: openai.ChatCompletionMessageToolCallFunction{
					Name:      tc.Function.Name,
					Arguments: tc.Function.Arguments, // já é string no seu SDK
				},
			}

			// Atribuir Type de forma robusta (string, *string ou tipo definido)
			rv := reflect.ValueOf(&item).Elem()
			f := rv.FieldByName("Type")
			if f.IsValid() && f.CanSet() {
				switch f.Kind() {
				case reflect.String:
					f.SetString(tc.Type)
				case reflect.Ptr:
					if f.Type().Elem().Kind() == reflect.String {
						s := tc.Type
						f.Set(reflect.ValueOf(&s))
					} else {
						if reflect.TypeOf(tc.Type).AssignableTo(f.Type()) {
							f.Set(reflect.ValueOf(tc.Type))
						}
					}
				default:
					if reflect.TypeOf(tc.Type).AssignableTo(f.Type()) {
						f.Set(reflect.ValueOf(tc.Type))
					}
				}
			}

			finalMessage.ToolCalls = append(finalMessage.ToolCalls, item)
		}
	}

	// checar erro do stream
	if stream.Err() != nil {
		return nil, stream.Err()
	}

	reqElapsed := time.Since(reqStart)

	finalMessage.Content = collectedContent.String()

	totalElapsed := time.Since(startReq)
	log.Debug().
		Interface("final_message", finalMessage).
		Dur("request_elapsed", reqElapsed).
		Dur("total_elapsed", totalElapsed).
		Msg("📥 Final AI message assembled from stream")

	updatedMessages := append(messages, finalMessage.ToParam())

	// se não tem tool calls, retorna
	if len(finalMessage.ToolCalls) == 0 {
		log.Info().
			Str("user_id", userID).
			Msg("ℹ️ No tool calls made, proceeding with streaming")
		return updatedMessages, nil
	}

	// Para cada tool call: encontrar handler, executar e anexar ToolMessage
	for _, toolCall := range finalMessage.ToolCalls {
		log.Info().
			Str("user_id", userID).
			Str("tool_name", toolCall.Function.Name).
			Str("tool_id", toolCall.ID).
			Msg("🔄 Processing tool call")

		// encontrar handler
		var handler ToolHandler
		for _, tool := range c.tools {
			if tool.Definition.Function.Name == toolCall.Function.Name {
				handler = tool.Handler
				break
			}
		}

		if handler == nil {
			log.Error().
				Str("user_id", userID).
				Str("tool_name", toolCall.Function.Name).
				Msg("❌ No handler found for tool")
			failPayload := map[string]any{
				"error": "no handler found for tool: " + toolCall.Function.Name,
			}
			// converter para string JSON antes de passar ao ToolMessage
			if b, err := json.Marshal(failPayload); err == nil {
				updatedMessages = append(updatedMessages, openai.ToolMessage(string(b), toolCall.ID))
			} else {
				updatedMessages = append(updatedMessages, openai.ToolMessage("no handler found", toolCall.ID))
			}
			continue
		}

		// parse argumentos (arguments é string JSON)
		var args map[string]any
		if toolCall.Function.Arguments != "" {
			if err := json.Unmarshal([]byte(toolCall.Function.Arguments), &args); err != nil {
				log.Error().
					Err(err).
					Str("user_id", userID).
					Str("tool_name", toolCall.Function.Name).
					Msg("❌ Failed to parse tool arguments")
				warnPayload := map[string]any{
					"error":    "failed to parse tool arguments",
					"rawArgs":  toolCall.Function.Arguments,
					"toolName": toolCall.Function.Name,
				}
				if b, err := json.Marshal(warnPayload); err == nil {
					updatedMessages = append(updatedMessages, openai.ToolMessage(string(b), toolCall.ID))
				} else {
					updatedMessages = append(updatedMessages, openai.ToolMessage("failed to parse tool args", toolCall.ID))
				}
				continue
			}
		} else {
			args = map[string]any{}
		}

		// executar handler com timeout
		handlerCtx, cancel := context.WithTimeout(ctx, 10*time.Second)
		resultCh := make(chan any, 1)
		errCh := make(chan error, 1)

		go func() {
			res, err := handler(handlerCtx, args)
			if err != nil {
				errCh <- err
				return
			}
			resultCh <- res
		}()

		var result any
		select {
		case <-handlerCtx.Done():
			cancel()
			log.Error().
				Str("user_id", userID).
				Str("tool_name", toolCall.Function.Name).
				Msg("❌ Tool handler timed out")
			return nil, fmt.Errorf("tool handler timed out: %s", toolCall.Function.Name)
		case err := <-errCh:
			cancel()
			userMsg := err.Error()
			log.Error().
				Err(err).
				Str("user_id", userID).
				Str("tool_name", toolCall.Function.Name).
				Msg("❌ Tool handler returned error")
			return nil, &aiutils.ToolCallUserError{UserMessage: userMsg, Err: err}
		case r := <-resultCh:
			result = r
			cancel()
		}

		// --- Converter o resultado para um tipo aceito por openai.ToolMessage ---
		// aceitamos:
		//  - string
		//  - []openai.ChatCompletionContentPartTextParam
		// caso contrário, serializamos para JSON string.

		switch v := result.(type) {
		case string:
			updatedMessages = append(updatedMessages, openai.ToolMessage(v, toolCall.ID))
		case []openai.ChatCompletionContentPartTextParam:
			updatedMessages = append(updatedMessages, openai.ToolMessage(v, toolCall.ID))
		case []byte:
			updatedMessages = append(updatedMessages, openai.ToolMessage(string(v), toolCall.ID))
		default:
			// tentar serializar qualquer coisa para JSON
			if b, err := json.Marshal(v); err == nil {
				updatedMessages = append(updatedMessages, openai.ToolMessage(string(b), toolCall.ID))
			} else {
				// fallback — passar uma string simples
				updatedMessages = append(updatedMessages, openai.ToolMessage("tool returned non-serializable result", toolCall.ID))
			}
		}
	}

	return updatedMessages, nil
}


// messageWithIndex wraps a message with its index for ordered processing
type messageWithIndex struct {
	message Message
	index   int
}

// sendMessagesSequentially processes messages from the queue one at a time to ensure ordering
func (c *Client) sendMessagesSequentially(
	ctx context.Context,
	config streamingConfig,
	messageQueue <-chan messageWithIndex,
) {
	isFirstMessage := true
	messagesProcessed := 0

	log.Info().
		Str("user_id", config.userID).
		Msg("Starting sequential message processing goroutine")

	defer func() {
		log.Info().
			Str("user_id", config.userID).
			Int("total_messages_processed", messagesProcessed).
			Msg("Sequential message processing goroutine finished")
	}()

	for {
		select {
		case msgWithIndex, ok := <-messageQueue:
			if !ok {
				log.Info().
					Str("user_id", config.userID).
					Int("messages_processed", messagesProcessed).
					Msg("Message queue closed, finishing sequential processing")
				return
			}

			msg := msgWithIndex.message
			messageIndex := msgWithIndex.index

			log.Info().
				Str("user_id", config.userID).
				Int("message_index", messageIndex).
				Int("messages_processed_so_far", messagesProcessed).
				Str("content", msg.Content).
				Str("type", msg.Type).
				Msg("Processing message from queue")

			if !isFirstMessage {
				log.Debug().
					Str("user_id", config.userID).
					Msg("Applying 500ms delay between messages")

				select {
				case <-time.After(500 * time.Millisecond):
				case <-ctx.Done():
					log.Info().
						Str("user_id", config.userID).
						Msg("Context cancelled during debounce delay")
					return
				}
			}
			isFirstMessage = false

			// Enviar a mensagem
			if msg.Type == "audio" {
				if err := c.sendAudioMessage(ctx, config, msg, messageIndex); err != nil {
					log.Error().
						Err(err).
						Str("user_id", config.userID).
						Int("message_index", messageIndex).
						Msg("Failed to send audio message, continuing with next")
				} else {
					messagesProcessed++
				}
			} else {
				if err := c.sendTextMessage(ctx, config, msg, messageIndex); err != nil {
					log.Error().
						Err(err).
						Str("user_id", config.userID).
						Int("message_index", messageIndex).
						Msg("Failed to send text message, continuing with next")
				} else {
					messagesProcessed++
				}
			}

		case <-ctx.Done():
			log.Info().
				Str("user_id", config.userID).
				Int("messages_processed", messagesProcessed).
				Msg("Context cancelled, stopping sequential message processing")
			return
		}
	}
}

// Funções auxiliares para melhor organização e logs
func (c *Client) sendAudioMessage(
	_ context.Context,
	config streamingConfig,
	msg Message,
	messageIndex int,
) error {
	log.Info().
		Str("user_id", config.userID).
		Int("message_index", messageIndex).
		Str("content", msg.Content).
		Msg("Converting text to speech")

	audioURL, err := config.elevenLabsClient.ConvertTextToSpeechDefault(msg.Content)
	if err != nil {
		log.Error().
			Err(err).
			Str("user_id", config.userID).
			Str("content", msg.Content).
			Int("message_index", messageIndex).
			Msg("Error converting text to speech")
		return err
	}

	log.Info().
		Str("user_id", config.userID).
		Int("message_index", messageIndex).
		Str("audio_url", audioURL).
		Msg("Sending audio message to Vonage")

	response, err := config.vonageClient.SendWhatsAppAudioMessage(
		config.toNumber,
		audioURL,
	)
	if err != nil {
		log.Error().
			Err(err).
			Str("user_id", config.userID).
			Str("to", config.toNumber).
			Str("audio_url", audioURL).
			Int("message_index", messageIndex).
			Msg("Error sending WhatsApp audio message to Vonage")
		return err
	}

	log.Info().
		Str("user_id", config.userID).
		Str("message_uuid", response.MessageUUID).
		Str("audio_url", audioURL).
		Int("message_index", messageIndex).
		Msg("Successfully sent audio message via Vonage")

	return nil
}

func (c *Client) sendTextMessage(
	_ context.Context,
	config streamingConfig,
	msg Message,
	messageIndex int,
) error {
	log.Info().
		Str("user_id", config.userID).
		Int("message_index", messageIndex).
		Str("content", msg.Content).
		Msg("Sending text message to Vonage")

	response, err := config.vonageClient.SendWhatsAppTextMessage(
		config.toNumber,
		msg.Content,
	)
	if err != nil {
		log.Error().
			Err(err).
			Str("user_id", config.userID).
			Str("to", config.toNumber).
			Str("content", msg.Content).
			Int("message_index", messageIndex).
			Msg("Error sending WhatsApp text message to Vonage")
		return err
	}

	log.Info().
		Str("user_id", config.userID).
		Str("message_uuid", response.MessageUUID).
		Str("content", msg.Content).
		Int("message_index", messageIndex).
		Msg("Successfully sent text message via Vonage")

	return nil
}

// finalizeStreamingResponse validates the final JSON response and stores it in Redis.
// It ensures the complete response is properly formatted and saved for chat history.
func (c *Client) finalizeStreamingResponse(
	userID string,
	fullContent string,
	redisClient *redis.Client,
) error {
	log.Info().
		Str("user_id", userID).
		Int("content_length", len(fullContent)).
		Msg("Finalizing streaming response - validating JSON")

	// Debug: show the complete JSON being parsed
	fmt.Printf("DEBUG: Complete JSON being parsed: %q\n", fullContent)

	var messageList MessageList
	if err := json.Unmarshal([]byte(fullContent), &messageList); err != nil {
		log.Error().
			Err(err).
			Str("user_id", userID).
			Str("content", fullContent).
			Msg("Error parsing final JSON response")
		return err
	}

	log.Info().
		Str("user_id", userID).
		Int("message_count", len(messageList.Messages)).
		Msg("Successfully parsed JSON response")

	allMessagesContent := []string{}
	for i, msg := range messageList.Messages {
		allMessagesContent = append(allMessagesContent, msg.Content)
		log.Debug().
			Str("user_id", userID).
			Int("message_index", i).
			Str("content", msg.Content).
			Str("type", msg.Type).
			Msg("Processing message for Redis storage")
	}
	fullResponse := strings.Join(allMessagesContent, "\n\n")

	log.Info().
		Str("user_id", userID).
		Int("final_response_length", len(fullResponse)).
		Msg("Storing bot message in Redis")

	if err := redisClient.AddBotMessage(userID, fullResponse); err != nil {
		log.Error().
			Err(err).
			Str("user_id", userID).
			Msg("Error storing bot message in Redis")
		return err
	}

	log.Info().
		Str("user_id", userID).
		Msg("Successfully stored bot message in Redis")

	return nil
}
